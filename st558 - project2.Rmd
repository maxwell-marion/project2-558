---
title: "ST558 - Project 2"
author: 'Group 9: Max Marion-Spencer & Xin Wang'
date: '2022-07-03'
output: html_document
params: 
      data_channel: bus
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,message=FALSE,warning=FALSE}
library(tidyverse)
library(tibble)
library(caret)
library(gbm)
library(knitr)
library(dplyr)
library(ggplot2)
library(corrplot)
library(vtable)
```

## Introduction 

For this project, we will be using four models (two Linear Regression, one Random Forest and one Boosted Tree) to make predictions on the number of shares using the Online News Popularity dataset. This dataset contains six different data channels, variables we choose for analysis are: 

* Number of words in the title  
* Number of words in the content  
* Number of links  
* Number of images  
* Number of videos  
* Number of keywords in the metadata  
* Article published Day in a week  
* Article published on the weekend or not 

The response variable for the dataset is:

* Number of Shares

We will randomly split the data into a training (70%) and a test (30%) set and fit four models on the training set. We will then fit four models on the test set and decide which model produces the best prediction, which is the one with the smallest root mean squared error.

## Data

```{r}
# read in data and select varaibles
news <- read_csv("OnlineNewsPopularity.csv", show_col_types = FALSE) %>% select(c(3,4,8,10,11,13,14:19,32:39,61))
colnames(news)
news
```

```{r}
# pivot data channel from wide to long 
news <- news %>%
  pivot_longer(
    cols = starts_with("data_channel"),
    names_to = "data_channel",
    values_to = "data_channel_type"
  ) %>%
  pivot_longer(
    cols = starts_with("weekday_is"),
    names_to = "weekday",
    values_to = "weekday_type"
  )

# remove prefix and filter both type with 1
news<- news %>% 
  mutate(weekday = str_sub(weekday, 12, -1),
          data_channel = str_sub(data_channel, 17, -1)) %>% 
  filter(data_channel_type == 1 & weekday_type ==1)
```

```{r}
# factor weekday and data channel
news$weekday <- as.factor(news$weekday)
news$weekday<- ordered(news$weekday, levels=c("monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday"))
levels(news$weekday)

news$data_channel <- as.factor(news$data_channel)
levels(news$data_channel)
```

```{r}
# filter out one data channel 
news <- news %>% filter(data_channel == params$data_channel)
news
#summary(news)
```

```{r}
# splitting data into training & test sets
set.seed(11)
train <- sample(1:nrow(news),size=nrow(news)*0.7)
test <- setdiff(1:nrow(news),train)
newsTrain <- news[train, ]
newsTest <- news[test, ]
```

## Summarizations

```{r}
# Building summary statistics for our data
# NOTE: Using a standard output instead of kable, as the column names get
# grouped too closely together otherwise
news_summary <- sumtable(news[0:6], out = "return", add.median = TRUE)
news_summary

```


### Contingency Tables

This table shows the count of days in a week for article publish. It helps us to see which day of the week article published the most and whether there are more article published on the weekdays or weekends. 

```{r}
kable(t(table(newsTrain$weekday)), caption = "Article Published Day")
```

We transform shares into a binary variable using the decision threshold of 1,400. This contingency table shows the count of shares on the weekdays and weekends. 

```{r}
newsTrain <- newsTrain %>% 
  mutate(SharesClass = if_else(shares >=1400, "More than 1,400","Less than 1,400"), 
         is_weekend = if_else(is_weekend== 0, "Weekday", "Weekend"))

kable(table(newsTrain$SharesClass,newsTrain$is_weekend), caption = "Count of Shares")
```

### Bar Plot

In this bar plot, we can see the count and the comparison of shares less than 1,400 and shares more than 1,400 in a week, where longer bars indicate higher values.

```{r}
ggplot(newsTrain, aes(x = weekday)) + 
  geom_bar(aes(fill = SharesClass), position = "dodge") + 
  labs(x = "Week") + 
  scale_fill_discrete(name = "Shares") + 
  ggtitle("Shares by Week")
```

### density plot 

This density plot shows the distribution for number of shares on weekdays and on weekends. Higher peak indicates there is a higher concentration of points. If the curve is left skewed, then the mean is less than the median. If the curve is right skewed, then the mean is greater than the median. If the curve has no skew, then the mean is equal to the median.

```{r}
# outlier ??
ggplot(newsTrain, aes(x = shares)) + 
  geom_density(alpha = 0.5, position = "stack", aes(fill = is_weekend)) +
  labs(title = "Density Plot for Shares") +
  scale_fill_discrete(name = "Week", labels = c("Weekday", "Weekend"))  
```

### Correlation Plot

The correlation plot shows the correlation between all numeric predictor variables. If the color is close to dark blue means there is a positive correlation between two variables, if the color is close to dark red means there is a negative correlation between two variables.

```{r}
cor.variables <- newsTrain %>% 
  select(n_tokens_title, n_tokens_content, num_hrefs, num_imgs, num_videos,num_keywords)
correlation <- cor(cor.variables, method = "spearman")
corrplot(correlation)
```

### Scatter Plot: n_tokens_content vs. num_imgs

```{r}
# Creating a scatterplot of the number of words vs number of images
# NOTE: Adding some jitter to better convey where the majority of points lay
img_scatter <- ggplot(newsTrain, aes(x = n_tokens_content, y = num_imgs))
img_scatter + geom_point(aes(color = is_weekend, shape = SharesClass), position = "jitter") + geom_smooth(method = lm) +
  labs(title = "Image Count vs. Word Count", x = "Word Count", y = "Image Count", color = "Weekend or Weekday", shape = "Shares")
```


### Scatter Plot: n_tokens_content vs. num_videos

```{r}
# Creating a scatterplot of the number of words vs number of videos
vid_scatter <- ggplot(newsTrain, aes(x = n_tokens_content, y = num_videos))
vid_scatter + geom_point(aes(color = is_weekend, shape = SharesClass), position = "jitter") + geom_smooth(method = lm) +
  labs(title = "Image Count vs. Video Count", x = "Word Count", y = "Video Count", color = "Weekend or Weekday", shape = "Shares") 
```


### Boxplot of the Title Word Count vs. Share Category (Above 1,400 or Below 1,400)

```{r}
# Creating a boxplot of num title words vs share category
title_box <- ggplot(newsTrain, aes(x = SharesClass, y = n_tokens_title))
title_box + geom_boxplot() + 
  labs(title = "Title Word Count Boxplots By Share Category", 
       x = "Share Count", y = "Title Word Count")

```


## Modeling

Our first two models are linear regression models.

Linear regression modelling allows you to examine the relationship between a response variable and one or more predictor variables typically by minimizing the distance between the distance between your data points and a line (least squares regression.) 

We will be examining two relatively basic linear models, one with only our very basic predictor variables, and the other with all of our numeric predictor variables. 


### Linear Regression Model 1: Only basic numeric variables

This linear regression model contains just the most basic information about the
piece - the number of words in the title, the number of words in the piece, and
the number of both images and videos.


```{r}
fit.lr1 <- train(shares ~ n_tokens_title + n_tokens_content + num_imgs + num_videos, 
                 data = newsTrain,
                 method = "lm",
                 preProcess = c("center", "scale"),
                 trControl = trainControl(method = "cv", number = 10))
fit.lr1 

# on test set
pred1 <- round(postResample(predict(fit.lr1, newdata = newsTest), obs = newsTest$shares),3)
pred1
```


### Linear Regression Model 2: All numeric variables

This linear regression model contains all of our numeric variables, including links
and keywords.

```{r}
fit.lr2 <- train(shares ~ n_tokens_title + n_tokens_content + num_hrefs + num_imgs + num_videos + num_keywords, 
                 data = newsTrain,
                 method = "lm",
                 preProcess = c("center", "scale"),
                 trControl = trainControl(method = "cv", number = 10))
fit.lr2 

# on test set
pred2 <- round(postResample(predict(fit.lr2, newdata = newsTest), obs = newsTest$shares),3)
pred2
```

### Random Forest Model

Our random forest model is built on the concept of using multiple trees from bootstrap samples, and then averaging the results in order to reduce the susceptibility to variance that single trees have.

```{r}
fit.random <- train(shares ~ n_tokens_title + n_tokens_content + num_hrefs + num_imgs + num_videos + num_keywords,
                    method = "rf",
                    preProcess = c("center","scale"),
                    trControl = trainControl(method = "cv", number = 5),
                    tuneGrid = data.frame(mtry = 1:15),
                    data = newsTrain)
fit.random

# on test set
pred3 <- round(postResample(predict(fit.random, newdata = newsTest), obs = newsTest$shares),3)
pred3
```


### Boosted Tree Model

The boosted tree model is a tree based method where the trees grow in a sequential manner, each subsequent tree is based on the previous tree so it updates the prediction as the tree grows. Boosted tree model slowly trains the trees to ensure there is no overfitting for the training data. There are four tuning parameters: 

* n.trees: Number of trees, increasing n reduces the error on training set  
* interaction.depth: number of splits it has to perform on a tree
* shrinkage: learning rate, reduce the impact of each additional fitted tree 
* n.minobsinnode: minimum number of observations in trees' terminal nodes


```{r}
# tuning??
fit.boost <- train(shares ~ n_tokens_title + n_tokens_content + num_hrefs + num_imgs + num_videos + num_keywords,
                 data = newsTrain,
                 method = "gbm",
                 preProcess = c("center", "scale"),
                 trControl = trainControl(method = "cv", number = 10),
                 tuneGrid = expand.grid(n.trees = c(50, 100, 150),
                                        interaction.depth = 1:3, 
                                        shrinkage = 0.1, 
                                        n.minobsinnode = 10),
                 verbose = FALSE)
fit.boost

# on test set
pred4 <- round(postResample(predict(fit.boost, newdata = newsTest), obs = newsTest$shares),3)
pred4
```

## Comparison

```{r}
compare <- data.frame(pred1, pred2 , pred4)
colnames(compare) <- c("basic linear regression","linear regression","boosted tree")
compare
```
```{r}
# Building a quick compare function
compare_func <- function(compare){
  temp <- names(which.min(compare[1,]))
  return(cat("We have selected our",temp,"model- as this is the model with the lowest RMSE."))
}
```

```{r}
compare_func(compare)
```


## Automation


this should work, can test after finished above 

```{r,eval=FALSE}
data_channel <- c("bus","entertainment","lifestyle","socmed","tech","world")
output_file <- paste0(data_channel, "Analysis.md")
params = lapply(data_channel, 
                FUN = function(x){
                  list(data_channel = x)})
reports <- tibble(output_file, params)
apply(reports, 
      MARGIN = 1,
      FUN = function(x){
        rmarkdown::render(input = "st558 - project2.Rmd", 
                          output_format = "github_document", 
                          output_options = list(html_preview = FALSE), 
                          output_file = x[[1]], 
                          params = x[[2]])})
```

